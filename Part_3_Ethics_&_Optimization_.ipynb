{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfWzGoQPvhqo3xJi1UkUVj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimanirobbi/wk-3-ai/blob/main/Part_3_Ethics_%26_Optimization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Ethical Considerations and Bias Mitigation\n",
        "\n",
        " ## MNIST Handwritten Digits Model (Task 2)\n",
        "\n",
        " Potential Bias Identification:\n",
        "\n",
        "1. **Imbalance in Writing Style:** The MNIST dataset is primarily collected from high school students and U.S. Census Bureau employees. This sample is not globally representative.\n",
        "- **Bias:** The model might perform poorly on handwritten digits from individuals in other demographics (e.g., non-native English speakers, different cultural writing styles, or older adults with tremors). The model could inadvertently perform \"better\" for younger, American hands.\n",
        "\n",
        "2. **Stroke Width/Tool Bias:** The model is trained on standard pen/pencil strokes.\n",
        "- **Bias:** If deployed in an application that processes digits written with unusual tools (e.g., stylus, thick marker, finger drawing), the accuracy drop may disproportionately affect users who rely on these non-standard methods (e.g., users with motor impairments or those using older touch technology).\n",
        "\n",
        " **Mitigation using TensorFlow Fairness Indicators:**\n",
        "\n",
        " TensorFlow Fairness Indicators (TFFI) is a suite of tools that allows for the evaluation of common fairness metrics (e.g., demographic parity, equalized odds) across user-defined groups or \"slices.\"\n",
        "\n",
        "- **How TFFI Mitigates:**\n",
        "\n",
        "  1. **Define Slices**:Though the MNIST metadata is limited, one could create a proxy for writing style bias by slicing the test data based on features like the average stroke thickness or the overall size of the bounding box.\n",
        "\n",
        "  2. **Evaluate Performance by Slice**:TFFI would be used to compare the accuracy, precision, and recall metrics for different slices (e.g., 'thin-stroke' vs. 'thick-stroke' groups).\n",
        "\n",
        "  3. **Action**: If TFFI shows a significant drop in recall for the 'thick-stroke' group, it indicates a bias. The mitigation step would be to augment the training data with synthetic \"thick-stroke\" examples or assign higher loss weights to samples from the underperforming group during training.\n",
        "\n",
        "## Amazon Product Reviews Model (Task 3)\n",
        "\n",
        "Potential Bias Identification:\n",
        "\n",
        "1. **Lexical Bias in Sentiment:** A simple rule-based system (like the one implemented) is highly susceptible to lexical bias.\n",
        "- **Bias:** If a positive word is common in reviews for a specific product only used by a certain demographic (e.g., \"fast\" in gaming PC reviews, predominantly male users), and a neutral word is common in another demographic's product reviews, the model's confidence scores can inadvertently associate certain language with higher sentiment, reinforcing the language patterns of the majority/dominant user group.\n",
        "\n",
        "2. **Cultural/Linguistic Bias:** Positive words in one language/dialect (e.g., \"wicked\" for good) might be misclassified if the rule-set is purely based on standard English.\n",
        "\n",
        "**Mitigation using spaCy's Rule-Based Systems:**\n",
        "\n",
        "Rule-based systems are flexible and can be directly used to mitigate identified biases:\n",
        "\n",
        "- **How spaCy Rules Mitigate:**\n",
        "\n",
        "1. **Bias Detection:** If analysis shows that reviews discussing 'Product X' (often associated with Group A) are being systematically downgraded, review the sentiment rule dictionary.\n",
        "2. **Rule Refinement (Mitigation):** You can use spaCy's Matcher to identify specific phrase patterns or product-associated jargon and assign explicit sentiment scores to them, overriding the general rule-set. For example, add rules like:\n",
        "\n",
        "- {\"LOWER\": \"price\"}, {\"LOWER\": \"too\"}, {\"LOWER\": \"high\"} -> Strong NEGATIVE.\n",
        "\n",
        "- {\"TEXT\": \"seamless\"}, {\"TEXT\": \"integration\"} -> Strong POSITIVE.\n",
        "\n",
        "3. **Stopword Expansion:** Add product-specific neutral terms or common idioms (like \"all things considered\") to a custom stopword list or a dictionary of neutral terms to prevent them from accidentally tilting the score. This manual, transparent refinement ensures the rules are culturally and contextually fair to all user segments."
      ],
      "metadata": {
        "id": "KWO_XwYs_K1f"
      }
    }
  ]
}